use tauri::{AppHandle, Emitter};
use serde::Serialize;
use std::process::{Command, Stdio};

#[derive(Serialize, Clone)]
struct ComponentLog {
    component: String,
    message: String,
}

#[tauri::command]
pub fn install_ollama_real(app: AppHandle) -> Result<(), String> {
    app.emit("component-log", ComponentLog {
        component: "AI Brain".into(),
        message: "ðŸ§  Checking Ollama installation...".into(),
    }).ok();

    // --- Check if Ollama is already installed ---
    let check = Command::new("ollama")
        .arg("--version")
        .stdout(Stdio::piped())
        .stderr(Stdio::null())
        .output();

    match check {
        Ok(out) if out.status.success() => {
            let version = String::from_utf8_lossy(&out.stdout).trim().to_string();
            app.emit("component-log", ComponentLog {
                component: "AI Brain".into(),
                message: format!("âœ… Ollama detected ({})", version),
            }).ok();
            return Ok(());
        }
        _ => {
            // --- Not found â€” ask user to install manually ---
            app.emit("component-log", ComponentLog {
                component: "AI Brain".into(),
                message: "âš  Ollama not found on this system.".into(),
            }).ok();
            app.emit("component-log", ComponentLog {
                component: "AI Brain".into(),
                message: "ðŸ’¡ Please download Ollama manually from https://ollama.com/download/windows".into(),
            }).ok();
            app.emit("component-log", ComponentLog {
                component: "AI Brain".into(),
                message: "â¬† Once installed, click 'Check Again' in the installer to continue.".into(),
            }).ok();
            return Err("Ollama not found â€” waiting for manual installation".into());
        }
    }
}
