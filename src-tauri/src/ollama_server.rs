use tauri::{AppHandle, Emitter};
use serde::{Deserialize, Serialize};
use std::process::{Command, Stdio};
use std::io::{BufRead, BufReader};
use std::thread;
use std::path::Path;
use std::time::{Duration, Instant};
use std::sync::{Arc, Mutex};
use std::net::TcpStream;
use std::env;
use once_cell::sync::Lazy;

use crate::config::AppConfig;

#[derive(Serialize, Clone)]
struct ComponentLog {
    component: String,
    message: String,
}

// === Global Handles ===
static OLLAMA_PROCESS: Lazy<Arc<Mutex<Option<std::process::Child>>>> =
    Lazy::new(|| Arc::new(Mutex::new(None)));

static OLLAMA_DOWNLOAD: Lazy<Arc<Mutex<Option<std::process::Child>>>> =
    Lazy::new(|| Arc::new(Mutex::new(None)));

/// üöÄ Start Ollama server
#[tauri::command]
pub fn start_ollama_server(app: AppHandle) -> Result<(), String> {
    let component = "Ollama Server";
    app.emit(
        "component-log",
        ComponentLog {
            component: component.into(),
            message: "üöÄ Attempting to start Ollama server...".into(),
        },
    )
    .ok();

    let config = AppConfig::load();
    let ollama_port = config.ollama_port.unwrap_or(11434);
    let ollama_path =
        detect_ollama_path().ok_or("‚ùå Ollama binary not found on this system.")?;

    app.emit(
        "component-log",
        ComponentLog {
            component: component.into(),
            message: format!("üìÇ Ollama binary located at '{}'", ollama_path),
        },
    )
    .ok();

    // üßπ Free port if busy
    let port = config.ollama_port.unwrap_or(11434);
    if is_listening(port) {
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: format!("‚ö† Port {} is already in use. Attempting to free it...", port),
            },
        )
        .ok();
        let _ = free_port(port);
        thread::sleep(Duration::from_secs(2));
    }

    if is_listening(ollama_port) {
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: format!("‚úÖ Ollama already running on port {}", ollama_port),
            },
        )
        .ok();
        return Ok(());
    }

    let mut cmd = Command::new(&ollama_path)
        .arg("serve")
        .stdout(Stdio::piped())
        .stderr(Stdio::piped())
        .spawn()
        .map_err(|e| format!("‚ùå Failed to start Ollama server: {}", e))?;

    let stdout = cmd.stdout.take().unwrap();
    let stderr = cmd.stderr.take().unwrap();

    let app_out = app.clone();
    let component_out = component.to_string();
    thread::spawn(move || {
        let reader = BufReader::new(stdout);
        for line in reader.lines().flatten() {
            app_out
                .emit(
                    "component-log",
                    ComponentLog {
                        component: component_out.clone(),
                        message: line,
                    },
                )
                .ok();
        }
    });

    let app_err = app.clone();
    let component_err = component.to_string();
    thread::spawn(move || {
        let reader = BufReader::new(stderr);
        for line in reader.lines().flatten() {
            app_err
                .emit(
                    "component-log",
                    ComponentLog {
                        component: component_err.clone(),
                        message: format!("‚ö† {}", line),
                    },
                )
                .ok();
        }
    });

    {
        let mut handle = OLLAMA_PROCESS.lock().unwrap();
        *handle = Some(cmd);
    }

    thread::sleep(Duration::from_secs(3));
    app.emit(
        "component-log",
        ComponentLog {
            component: component.into(),
            message: format!(
                "‚úÖ Ollama server started successfully on port {}",
                ollama_port
            ),
        },
    )
    .ok();

    Ok(())
}

/// üõë Stop Ollama server
#[tauri::command]
pub fn stop_ollama_server(app: AppHandle) -> Result<(), String> {
    let component = "Ollama Server";
    let mut handle = OLLAMA_PROCESS.lock().unwrap();

    if let Some(child) = handle.as_mut() {
        let _ = child.kill();
        *handle = None;
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: "üõë Ollama server stopped successfully.".into(),
            },
        )
        .ok();
    } else {
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: "‚Ñπ Ollama server was not running.".into(),
            },
        )
        .ok();
    }
    Ok(())
}

/// üì¶ List available models
#[tauri::command]
pub fn list_ollama_models() -> Result<Vec<String>, String> {
    let ollama_path = detect_ollama_path().ok_or("‚ùå Ollama binary not found.")?;
    let output = Command::new(&ollama_path)
        .arg("list")
        .output()
        .map_err(|e| format!("Failed to list models: {}", e))?;

    if !output.status.success() {
        return Err(format!(
            "‚ùå Failed to list models: {}",
            String::from_utf8_lossy(&output.stderr)
        ));
    }

    let result = String::from_utf8_lossy(&output.stdout)
        .lines()
        .map(|s| s.trim().to_string())
        .filter(|s| !s.is_empty())
        .collect::<Vec<_>>();

    Ok(result)
}

/// ‚¨á Pull model from Ollama registry (real-time JSON progress)
#[tauri::command]
pub fn pull_ollama_model(app: AppHandle, model_name: String) -> Result<(), String> {
    let component = "Ollama Model Pull";
    let ollama_path = detect_ollama_path().ok_or("‚ùå Ollama binary not found.")?;

    app.emit(
        "component-log",
        ComponentLog {
            component: component.into(),
            message: format!("‚¨á Starting download for model '{}'...", model_name),
        },
    )
    .ok();

    thread::spawn(move || {
        let cmd_result = if is_listening(11434) {
            Command::new("curl")
                .args([
                    "-N",
                    "-s",
                    "-X",
                    "POST",
                    "http://localhost:11434/api/pull",
                    "-H",
                    "Content-Type: application/json",
                    "-d",
                    &format!("{{\"name\":\"{}\"}}", model_name),
                ])
                .stdout(Stdio::piped())
                .stderr(Stdio::null())
                .spawn()
        } else {
            Command::new(&ollama_path)
                .args(["pull", &model_name])
                .stdout(Stdio::piped())
                .stderr(Stdio::null())
                .spawn()
        };

        let mut cmd = match cmd_result {
            Ok(c) => c,
            Err(e) => {
                let _ = app.emit(
                    "component-log",
                    ComponentLog {
                        component: component.into(),
                        message: format!("‚ùå Failed to start pull: {}", e),
                    },
                );
                return;
            }
        };

        let stdout = cmd.stdout.take();
        {
            let mut dl = OLLAMA_DOWNLOAD.lock().unwrap();
            *dl = Some(cmd);
        }

        if let Some(stdout) = stdout {
            let app_progress = app.clone();
            let comp = component.to_string();
            let model = model_name.clone();
            thread::spawn(move || {
                let reader = BufReader::new(stdout);
                let mut last_emit = Instant::now();
                let mut progress_seen = false;

                for line in reader.lines().flatten() {
                    if let Some(msg) = parse_ollama_json_line(&line) {
                        progress_seen = true;
                        if last_emit.elapsed() > Duration::from_secs(1) {
                            let _ = app_progress.emit(
                                "component-log",
                                ComponentLog {
                                    component: comp.clone(),
                                    message: msg,
                                },
                            );
                            last_emit = Instant::now();
                        }
                    }
                }

                if progress_seen {
                    let _ = app_progress.emit(
                        "component-log",
                        ComponentLog {
                            component: comp.clone(),
                            message: format!("‚úÖ Finished pulling '{}'", model),
                        },
                    );
                }
            });
        }

        let status = {
            let mut handle = OLLAMA_DOWNLOAD.lock().unwrap();
            if let Some(mut child) = handle.take() {
                child.wait()
            } else {
                return;
            }
        };

        match status {
            Ok(s) if s.success() => {
                let _ = app.emit(
                    "component-log",
                    ComponentLog {
                        component: component.into(),
                        message: format!("‚úÖ Model '{}' pulled successfully.", model_name),
                    },
                );
            }
            Ok(_) => {
                let _ = app.emit(
                    "component-log",
                    ComponentLog {
                        component: component.into(),
                        message:
                            "‚ùå Model pull failed. üí° Try the Repair Model Pull option.".into(),
                    },
                );
            }
            Err(e) => {
                let _ = app.emit(
                    "component-log",
                    ComponentLog {
                        component: component.into(),
                        message: format!("‚ùå Pull interrupted: {}", e),
                    },
                );
            }
        }
    });

    Ok(())
}

/// ‚èπ Cancel active model download
#[tauri::command]
pub fn cancel_ollama_download(app: AppHandle) -> Result<(), String> {
    let component = "Ollama Cancel Download";
    let mut handle = OLLAMA_DOWNLOAD.lock().unwrap();

    if let Some(child) = handle.as_mut() {
        let _ = child.kill();
        *handle = None;
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: "‚èπ Download cancelled by user.".into(),
            },
        )
        .ok();
    } else {
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: "‚Ñπ No active download to cancel.".into(),
            },
        )
        .ok();
    }

    Ok(())
}

/// üóë Remove model
#[tauri::command]
pub fn remove_ollama_model(app: AppHandle, model_name: String) -> Result<(), String> {
    let component = "Ollama Remove Model";
    let ollama_path = detect_ollama_path().ok_or("‚ùå Ollama binary not found.")?;

    let output = Command::new(&ollama_path)
        .args(["rm", &model_name])
        .output()
        .map_err(|e| format!("‚ùå Failed to remove model: {}", e))?;

    if output.status.success() {
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: format!("‚úÖ Model '{}' removed successfully.", model_name),
            },
        )
        .ok();
    } else {
        app.emit(
            "component-log",
            ComponentLog {
                component: component.into(),
                message: format!(
                    "‚ùå Removal failed: {}",
                    String::from_utf8_lossy(&output.stderr)
                ),
            },
        )
        .ok();
    }
    Ok(())
}

/// ‚ôª Repair model pull
#[tauri::command]
pub fn repair_ollama_model(app: AppHandle, model_name: String) -> Result<(), String> {
    let component = "Ollama Repair Pull";
    app.emit(
        "component-log",
        ComponentLog {
            component: component.into(),
            message: format!("üîÑ Attempting to repair pull for '{}'...", model_name),
        },
    )
    .ok();
    pull_ollama_model(app, model_name)
}

// === Helpers ===

#[derive(Deserialize)]
struct OllamaProgress {
    status: Option<String>,
    completed: Option<u64>,
    total: Option<u64>,
}

fn parse_ollama_json_line(line: &str) -> Option<String> {
    if let Ok(json) = serde_json::from_str::<OllamaProgress>(line) {
        if let (Some(c), Some(t)) = (json.completed, json.total) {
            let pct = ((c as f64 / t as f64) * 100.0) as u8;
            Some(format!("üì¶ Downloading model: {}% complete", pct))
        } else if let Some(status) = json.status {
            Some(format!("üì¶ {}", status))
        } else {
            None
        }
    } else if line.contains("pulling") {
        Some(format!("üì¶ {}", line))
    } else {
        None
    }
}

fn detect_ollama_path() -> Option<String> {
    let username = env::var("USERNAME").unwrap_or_default();
    let candidates = vec![
        "ollama".to_string(),
        r"C:\Program Files\Ollama\ollama.exe".to_string(),
        format!(
            r"C:\Users\{}\AppData\Local\Programs\Ollama\ollama.exe",
            username
        ),
        "/usr/local/bin/ollama".to_string(),
        "/usr/bin/ollama".to_string(),
    ];
    for c in candidates {
        if Path::new(&c).exists() {
            return Some(c);
        }
    }
    None
}

/// ‚úÖ Check if port is listening
fn is_listening(port: u16) -> bool {
    TcpStream::connect(("127.0.0.1", port)).is_ok()
}

/// üßπ Free the port if in use (cross-platform)
fn free_port(port: u16) -> Result<(), String> {
    #[cfg(target_os = "windows")]
    {
        let cmd = format!("netstat -ano | findstr :{}", port);
        let output = Command::new("cmd")
            .args(&["/C", &cmd])
            .output()
            .map_err(|e| format!("Failed to run netstat: {}", e))?;

        let stdout = String::from_utf8_lossy(&output.stdout).to_string();
        if stdout.trim().is_empty() {
            return Ok(());
        }

        for line in stdout.lines() {
            if let Some(pid) = line.split_whitespace().last() {
                let _ = Command::new("taskkill")
                    .args(&["/F", "/PID", pid])
                    .output()
                    .map_err(|e| format!("Failed to kill process on port {}: {}", port, e))?;
            }
        }
        Ok(())
    }

    #[cfg(not(target_os = "windows"))]
    {
        let output = Command::new("sh")
            .arg("-c")
            .arg(format!("lsof -t -i :{}", port))
            .output()
            .map_err(|e| format!("Failed to run lsof: {}", e))?;

        let stdout = String::from_utf8_lossy(&output.stdout).to_string();
        if stdout.trim().is_empty() {
            return Ok(());
        }

        for pid in stdout.lines() {
            let _ = Command::new("kill")
                .arg("-9")
                .arg(pid)
                .output()
                .map_err(|e| format!("Failed to kill PID {}: {}", pid, e))?;
        }
        Ok(())
    }
}
/// üß† Get current Ollama server status for frontend or n8n integration
#[tauri::command]
pub fn get_ollama_status() -> Result<String, String> {
    let cfg = AppConfig::load();
    let port = cfg.ollama_port.unwrap_or(11434);

    if TcpStream::connect(("127.0.0.1", port)).is_ok() {
        Ok(format!("‚úÖ Ollama is running on http://127.0.0.1:{}", port))
    } else {
        Err(format!("‚ùå Ollama not running on http://127.0.0.1:{}", port))
    }
}

/// üì¶ Return Ollama configuration details (for UI or n8n)
#[tauri::command]
pub fn get_ollama_details() -> Result<AppConfig, String> {
    Ok(AppConfig::load())
}
